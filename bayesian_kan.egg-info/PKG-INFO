Metadata-Version: 2.4
Name: bayesian-kan
Version: 1.0.0
Summary: Bayesian Kolmogorov-Arnold Networks with multiple inference methods
Home-page: https://github.com/HugSpike/bayesian-kan
Author: Bayesian KAN Team
Author-email: fengfan19990621@gmail.com
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: numpy>=1.20.0
Requires-Dist: matplotlib>=3.5.0
Requires-Dist: seaborn>=0.12.0
Requires-Dist: tqdm>=4.65.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: black>=22.0; extra == "dev"
Requires-Dist: flake8>=4.0; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Bayesian Kolmogorov-Arnold Networks (B-KAN)

A comprehensive implementation of Bayesian KAN with multiple inference methods including Variational Inference (VI), Markov Chain Monte Carlo (MCMC), and Laplace Approximation.

## Features

- **Bayesian KAN Architecture**: Probabilistic Kolmogorov-Arnold Networks with B-spline basis functions
- **Multiple Inference Methods**:
  - Variational Inference (VI) - Fast and scalable
  - MCMC with Hamiltonian Monte Carlo - Accurate posterior sampling
  - Laplace Approximation - Quick posterior approximation
- **Uncertainty Quantification**: Built-in prediction uncertainty estimation
- **Comprehensive Visualization**: Training curves, predictions with uncertainty bands, MCMC diagnostics
- **GPU Support**: Automatic GPU/CPU detection and usage

## Project Structure

```
bayesian_kan/
‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îú‚îÄ‚îÄ config.py                # Configuration and device setup
‚îú‚îÄ‚îÄ bspline.py              # B-spline basis functions
‚îú‚îÄ‚îÄ model.py                # Bayesian KAN model architecture
‚îú‚îÄ‚îÄ variational_inference.py # VI training
‚îú‚îÄ‚îÄ mcmc_sampling.py        # HMC sampling
‚îú‚îÄ‚îÄ laplace_approximation.py # Laplace approximation
‚îú‚îÄ‚îÄ visualization.py        # Visualization tools
‚îú‚îÄ‚îÄ utils.py                # Utility functions
‚îî‚îÄ‚îÄ examples/
    ‚îú‚îÄ‚îÄ example_vi.py       # VI example
    ‚îú‚îÄ‚îÄ example_mcmc.py     # MCMC example
    ‚îú‚îÄ‚îÄ example_laplace.py  # Laplace example
    ‚îî‚îÄ‚îÄ example_comparison.py # Compare all methods
```

## Installation

```bash
# Clone or download the repository
cd bayesian_kan

# Install dependencies
pip install torch numpy matplotlib seaborn tqdm
```

## Quick Start

### 1. Variational Inference (Recommended for most cases)

```python
from bayesian_kan import (
    BayesianKAN, VariationalInference, 
    generate_synthetic_data, create_data_loaders
)

# Generate data
X_train, y_train = generate_synthetic_data(n_samples=500)
train_loader, val_loader = create_data_loaders(X_train, y_train)

# Create model
model = BayesianKAN(layer_sizes=[1, 10, 10, 1])

# Train with VI
history = VariationalInference.train(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=100
)

# Make predictions with uncertainty
mean, std = model.predict_with_uncertainty(X_test, n_samples=100)
```

### 2. MCMC Sampling

```python
from bayesian_kan import MCMCSampler

# Run MCMC
mcmc_results = MCMCSampler.sample(
    model=model,
    data=(X_train, y_train),
    n_samples=1000,
    warmup=100
)

# Compute statistics
sample_stats = MCMCSampler.compute_statistics(mcmc_results['samples'])
```

### 3. Laplace Approximation

```python
from bayesian_kan import LaplaceApproximation

# Compute approximation
laplace_results = LaplaceApproximation.approximate(
    model=model,
    data=(X_train, y_train)
)

# Sample from posterior
samples = LaplaceApproximation.sample_from_posterior(
    map_params=laplace_results['map_params'],
    posterior_var=laplace_results['posterior_variance'],
    n_samples=100
)
```

## Running Examples

### Run individual examples:

```bash
# Variational Inference
python -m bayesian_kan.examples.example_vi

# MCMC Sampling
python -m bayesian_kan.examples.example_mcmc

# Laplace Approximation
python -m bayesian_kan.examples.example_laplace

# Compare all methods
python -m bayesian_kan.examples.example_comparison
```

## Method Comparison

| Method | Speed | Accuracy | Use Case |
|--------|-------|----------|----------|
| **VI** | ‚ö°‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê Good | General purpose, large datasets |
| **MCMC** | üêå Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | Small datasets, need accurate posterior |
| **Laplace** | ‚ö°‚ö° Medium | ‚≠ê‚≠ê‚≠ê Good | Quick approximation, medium datasets |

## Model Architecture

The Bayesian KAN uses:
- **B-spline basis functions** for smooth, learnable transformations
- **Variational parameters** (mean and log-variance) for each coefficient
- **KL divergence** regularization to stay close to prior
- **Uncertainty propagation** through the network

## Key Parameters

### Model Initialization
- `layer_sizes`: List of layer sizes (e.g., [1, 10, 10, 1])
- `n_basis`: Number of B-spline basis functions (default: 10)
- `degree`: B-spline degree (default: 3)
- `prior_scale`: Standard deviation of prior (default: 1.0)

### Training Parameters
- **VI**: 
  - `epochs`: Number of training epochs
  - `lr`: Learning rate
  - `kl_weight`: Weight for KL divergence term
- **MCMC**: 
  - `n_samples`: Number of samples to draw
  - `warmup`: Number of warmup iterations
  - `step_size`: HMC step size
- **Laplace**: 
  - `n_iterations`: MAP optimization iterations
  - `prior_weight`: Weight for prior term

## Visualization

The package includes comprehensive visualization tools:

```python
from bayesian_kan import BayesianKANVisualizer

viz = BayesianKANVisualizer(save_dir="results")

# Plot training history
viz.plot_training_history(history)

# Plot predictions with uncertainty
viz.plot_predictions_with_uncertainty(model, X_test, y_test)

# Plot parameter distributions
viz.plot_parameter_distributions(model)

# Plot MCMC diagnostics
viz.plot_mcmc_diagnostics(mcmc_results)

# Save results summary
viz.save_results_summary(model, history, metrics)
```

## Performance Tips

1. **Use VI for initial training**: It's fast and provides a good initialization
2. **Pre-train before MCMC**: Use VI to find a good starting point
3. **Adjust KL weight**: Lower for more flexible models, higher for more regularization
4. **Use smaller networks for MCMC**: Reduces computation time
5. **GPU acceleration**: Automatically used if available

## Citation

If you use this code in your research, please cite:

```bibtex
@software{bayesian_kan,
  title={Bayesian Kolmogorov-Arnold Networks},
  author={Bayesian KAN Team},
  year={2024},
  url={https://github.com/yourusername/bayesian-kan}
}
```

## License

MIT License

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## References

1. Kolmogorov, A. N. (1957). On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables.
2. Liu, Z., et al. (2024). KAN: Kolmogorov-Arnold Networks.
3. Blundell, C., et al. (2015). Weight Uncertainty in Neural Networks.
4. Neal, R. M. (2011). MCMC using Hamiltonian dynamics.
